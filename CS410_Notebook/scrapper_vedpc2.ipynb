{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping UIUC CS faculty homepages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System setup \n",
    "\n",
    "Before we start, make sure to install the required libraries\n",
    "    \n",
    "    pip install bs4\n",
    "    pip install selenium\n",
    "\n",
    "Since UIUC's website has some javascript rendered HTML content, we'll be using Selenium for scraping the content loaded dynamically by javascript. For this,you would also need to download a selenium supported browser webdriver.\n",
    "\n",
    "e.g. For Chrome, download the appropriate webdriver from here: http://chromedriver.chromium.org/downloads, unzip it and save in current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re \n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a webdriver object and set options for headless browsing\n",
    "options = Options()\n",
    "options.headless = True\n",
    "browser = webdriver.Chrome('./chromedriver',options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses webdriver object to execute javascript code and get dynamically loaded webcontent\n",
    "def get_js_soup(url,browser):\n",
    "    browser.get(url)\n",
    "    res_html = browser.execute_script('return document.body.innerHTML')\n",
    "    soup = BeautifulSoup(res_html,'html.parser') #beautiful soup object to be used for parsing html content\n",
    "    return soup\n",
    "\n",
    "#tidies extracted text \n",
    "def process_bio(bio):\n",
    "    bio = bio.encode('ascii',errors='ignore').decode('utf-8')       #removes non-ascii characters\n",
    "    bio = re.sub('\\s+',' ',bio)       #repalces repeated whitespace characters with single space\n",
    "    return bio\n",
    "\n",
    "''' More tidying\n",
    "Sometimes the text extracted HTML webpage may contain javascript code and some style elements. \n",
    "This function removes script and style tags from HTML so that extracted text does not contain them.\n",
    "'''\n",
    "def remove_script(soup):\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    return soup\n",
    "\n",
    "\n",
    "#Checks if bio_url is a valid faculty homepage\n",
    "def is_valid_homepage(bio_url,dir_url):\n",
    "    try:\n",
    "        #sometimes the homepage url points to the faculty profile page\n",
    "        #which should be treated differently from an actual homepage\n",
    "        ret_url = urllib.request.urlopen(bio_url).geturl() \n",
    "    except:\n",
    "        return False       #unable to access bio_url\n",
    "    urls = [re.sub('((https?://)|(www.))','',url) for url in [ret_url,dir_url]] #removes url scheme (https,http) or www \n",
    "    return not(urls[0]== urls[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts all Faculty Profile page urls from the Directory Listing Page\n",
    "def scrape_dir_page(dir_url,browser):\n",
    "    print ('-'*20,'Scraping directory page','-'*20)\n",
    "    faculty_links = []\n",
    "    faculty_base_url = 'https://cpsc.yale.edu'\n",
    "    #execute js on webpage to load faculty listings on webpage and get ready to parse the loaded HTML \n",
    "    soup = get_js_soup(dir_url,browser)     \n",
    "    for link_holder in soup.find_all('td',class_='views-field views-field-name'): #get list of all <div> of class 'photo nocaption'\n",
    "        rel_link = link_holder.find('a')['href'] #get url\n",
    "        #url returned is relative, so we need to add base url\n",
    "        faculty_links.append(faculty_base_url+rel_link) \n",
    "    print ('-'*20,'Found {} faculty profile urls'.format(len(faculty_links)),'-'*20)\n",
    "    return faculty_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Scraping directory page --------------------\n"
     ]
    },
    {
     "ename": "JavascriptException",
     "evalue": "Message: javascript error: Cannot read property 'innerHTML' of null\n  (Session info: headless chrome=79.0.3945.79)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJavascriptException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2acff11261cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdir_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://seas.yale.edu/faculty-research/faculty-directory'\u001b[0m \u001b[0;31m#url of directory listings of CS faculty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfaculty_links\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_dir_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-916c0b95f5fd>\u001b[0m in \u001b[0;36mscrape_dir_page\u001b[0;34m(dir_url, browser)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfaculty_base_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://seas.yale.edu/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#execute js on webpage to load faculty listings on webpage and get ready to parse the loaded HTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_js_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlink_holder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'viewmore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#get list of all <div> of class 'photo nocaption'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrel_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlink_holder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#get url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-ff6c975f7857>\u001b[0m in \u001b[0;36mget_js_soup\u001b[0;34m(url, browser)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_js_soup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mres_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'return document.body.innerHTML'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_html\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#beautiful soup object to be used for parsing html content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute_script\u001b[0;34m(self, script, *args)\u001b[0m\n\u001b[1;32m    634\u001b[0m         return self.execute(command, {\n\u001b[1;32m    635\u001b[0m             \u001b[0;34m'script'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m             'args': converted_args})['value']\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_async_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJavascriptException\u001b[0m: Message: javascript error: Cannot read property 'innerHTML' of null\n  (Session info: headless chrome=79.0.3945.79)\n"
     ]
    }
   ],
   "source": [
    "dir_url = 'https://cpsc.yale.edu/people/faculty' #url of directory listings of CS faculty\n",
    "faculty_links = scrape_dir_page(dir_url,browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_faculty_page(fac_url,browser):\n",
    "    soup = get_js_soup(fac_url,browser)\n",
    "    bio_url = ''\n",
    "    bio = ''\n",
    "    bio_header=''\n",
    "    bio_body=''\n",
    "    bio_header = soup.find('h1',class_='title').get_text(separator=' ')\n",
    "    bio_body = soup.find('div',class_='profile').get_text(separator=' ')\n",
    "    bio = bio_header + ' ' + bio_body\n",
    "    bio_url = fac_url\n",
    "    return bio_url,bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Scraping faculty url 1/25 --------------------\n",
      "-------------------- Scraping faculty url 2/25 --------------------\n",
      "-------------------- Scraping faculty url 3/25 --------------------\n",
      "-------------------- Scraping faculty url 4/25 --------------------\n",
      "-------------------- Scraping faculty url 5/25 --------------------\n",
      "-------------------- Scraping faculty url 6/25 --------------------\n",
      "-------------------- Scraping faculty url 7/25 --------------------\n",
      "-------------------- Scraping faculty url 8/25 --------------------\n",
      "-------------------- Scraping faculty url 9/25 --------------------\n",
      "-------------------- Scraping faculty url 10/25 --------------------\n",
      "-------------------- Scraping faculty url 11/25 --------------------\n",
      "-------------------- Scraping faculty url 12/25 --------------------\n",
      "-------------------- Scraping faculty url 13/25 --------------------\n",
      "-------------------- Scraping faculty url 14/25 --------------------\n",
      "-------------------- Scraping faculty url 15/25 --------------------\n",
      "-------------------- Scraping faculty url 16/25 --------------------\n",
      "-------------------- Scraping faculty url 17/25 --------------------\n",
      "-------------------- Scraping faculty url 18/25 --------------------\n",
      "-------------------- Scraping faculty url 19/25 --------------------\n",
      "-------------------- Scraping faculty url 20/25 --------------------\n",
      "-------------------- Scraping faculty url 21/25 --------------------\n",
      "-------------------- Scraping faculty url 22/25 --------------------\n",
      "-------------------- Scraping faculty url 23/25 --------------------\n",
      "-------------------- Scraping faculty url 24/25 --------------------\n",
      "-------------------- Scraping faculty url 25/25 --------------------\n"
     ]
    }
   ],
   "source": [
    "#Scrape all faculty homepages using profile page urls\n",
    "bio_urls, bios = [],[]\n",
    "tot_urls = len(faculty_links)\n",
    "for i,link in enumerate(faculty_links):\n",
    "    print ('-'*20,'Scraping faculty url {}/{}'.format(i+1,tot_urls),'-'*20)\n",
    "    bio_url,bio = scrape_faculty_page(link,browser)\n",
    "    bio_urls.append(bio_url)\n",
    "    bios.append(bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_lst(lst,file_):\n",
    "    with open(file_,'w') as f:\n",
    "        for l in lst:\n",
    "            f.write(l)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_urls_file = 'bio_urls.txt'\n",
    "bios_file = 'bios.txt'\n",
    "write_lst(bio_urls,bio_urls_file)\n",
    "write_lst(bios,bios_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
